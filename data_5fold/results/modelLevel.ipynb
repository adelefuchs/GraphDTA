{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93c458ad",
   "metadata": {},
   "source": [
    "how could i express model level ie overall cold start vulnerability. say for someone selecting between 2 models and they want to know which will be less vulnerable to cold start. \n",
    "\n",
    "define this only based off of davis and then show that see this model has a lower csv(as defined by my metric ) and does better on pharos than this other model that i also do\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b3eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point, Polygon\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "def compute_CHR_model_level_global(merged_df, min_points=3):\n",
    "    \"\"\"\n",
    "    Computes model-level CHR as a single value:\n",
    "    Fraction of all blinded points that fall outside\n",
    "    the convex hull of all unblinded points.\n",
    "    \"\"\"\n",
    "    points_unblinded = merged_df[[\"True_Label\", \"Unblinded_Prediction\"]].values\n",
    "    points_blinded = merged_df[[\"True_Label\", \"Blinded_Prediction\"]].values\n",
    "\n",
    "    if len(points_unblinded) < min_points:\n",
    "        raise ValueError(\"Not enough unblinded points to construct convex hull.\")\n",
    "\n",
    "    try:\n",
    "        hull = ConvexHull(points_unblinded)\n",
    "        polygon = Polygon(points_unblinded[hull.vertices])\n",
    "\n",
    "        inside_mask = np.array([polygon.contains(Point(p)) for p in points_blinded])\n",
    "        chr_value = (~inside_mask).sum() / len(points_blinded)\n",
    "        return chr_value\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Convex hull computation failed: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "\n",
    "def compute_CSPD_model_level_global(merged_df, min_affinity=5):\n",
    "    \"\"\"\n",
    "    Computes model-level CSPD as the mean absolute normalized prediction deviation (NPD)\n",
    "    across all points with True_Label â‰¥ min_affinity.\n",
    "    \"\"\"\n",
    "    df = merged_df.copy()\n",
    "    df = df[df[\"True_Label\"] >= min_affinity]\n",
    "\n",
    "    # Avoid division by zero\n",
    "    df = df[df[\"True_Label\"] != 0]\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No valid data points after applying min_affinity filter.\")\n",
    "\n",
    "    df[\"NPD\"] = (df[\"Unblinded_Prediction\"] - df[\"Blinded_Prediction\"]) / df[\"True_Label\"]\n",
    "    cspd_value = df[\"NPD\"].abs().mean()\n",
    "    return cspd_value\n",
    "\n",
    "\n",
    "\n",
    "def compute_CSVS_model_level_global(merged_df, min_affinity=5, alpha=None, beta=None):\n",
    "    \"\"\"\n",
    "    Computes model-level CSVS using global (non-grouped) CSPD and CHR.\n",
    "    Returns:\n",
    "        - scalar CSVS value\n",
    "        - CSPD scalar\n",
    "        - CHR scalar\n",
    "        - alpha, beta used\n",
    "    \"\"\"\n",
    "    # Get global CSPD and CHR\n",
    "    CSPD = compute_CSPD_model_level_global(merged_df, min_affinity)\n",
    "    CHR = compute_CHR_model_level_global(merged_df, min_affinity)\n",
    "\n",
    "    # Normalize weights\n",
    "    if alpha is None:\n",
    "        alpha = 1 / CSPD if CSPD != 0 else 0\n",
    "    if beta is None:\n",
    "        beta = 1 / CHR if CHR != 0 else 0\n",
    "\n",
    "    # Compute scalar CSVS\n",
    "    csvs_model = alpha * CSPD + beta * CHR\n",
    "\n",
    "    return csvs_model, CSPD, CHR, alpha, beta\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
